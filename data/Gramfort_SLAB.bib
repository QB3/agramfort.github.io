@unpublished{ablin:hal-02140383,
  TITLE = {{Learning step sizes for unfolded sparse coding}},
  AUTHOR = {Ablin, Pierre and Moreau, Thomas and Massias, Mathurin and Gramfort, Alexandre},
  URL = {https://hal.inria.fr/hal-02140383},
  NOTE = {working paper or preprint},
  YEAR = {2019},
  MONTH = May,
  PDF = {https://hal.inria.fr/hal-02140383/file/main.pdf},
  HAL_ID = {hal-02140383},
  HAL_VERSION = {v1},
}

@unpublished{massias:hal-02263500,
  TITLE = {{Dual Extrapolation for Sparse Generalized Linear Models}},
  AUTHOR = {Massias, Mathurin and Vaiter, Samuel and Gramfort, Alexandre and Salmon, Joseph},
  URL = {https://hal.archives-ouvertes.fr/hal-02263500},
  NOTE = {working paper or preprint},
  YEAR = {2019},
  MONTH = Aug,
  PDF = {https://hal.archives-ouvertes.fr/hal-02263500/file/main.pdf},
  HAL_ID = {hal-02263500},
  HAL_VERSION = {v1},
}

@InProceedings{janati-etal:2019b,
author = {Janati, H. and Bazeille, T. and Thirion, B. and Cuturi, M. and Gramfort, A.},
editor="Chung, Albert C. S.
and Gee, James C.
and Yushkevich, Paul A.
and Bao, Siqi",
title="Group Level MEG/EEG Source Imaging via Optimal Transport: Minimum Wasserstein Estimates",
booktitle="Information Processing in Medical Imaging",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="743--754",
isbn="978-3-030-20351-1",
pdf = {https://arxiv.org/pdf/1902.04812.pdf},
}

@ARTICLE{2019arXiv190202509B,
   author = {Bertrand, Q. and Massias, M. and Gramfort, A. and Salmon, J.},
    title = "{Concomitant Lasso with Repetitions (CLaR): beyond averaging multiple realizations of heteroscedastic noise}",
  journal = {arXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1902.02509},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Applications},
     year = 2019,
    month = feb,
   adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190202509B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  pdf = {https://arxiv.org/pdf/1902.02509.pdf},
}

@ARTICLE{2019arXiv190109235M,
   author = {Moreau, T. and Gramfort, A.},
    title = "{Distributed Convolutional Dictionary Learning (DiCoDiLe): Pattern Discovery in Large Images and Signals}",
  journal = {arXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1901.09235},
 keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning},
     year = 2019,
    month = jan,
   adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190109235M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  pdf = {https://arxiv.org/pdf/1901.09235.pdf},
}

@INPROCEEDINGS{ablin-etal:18c,
  TITLE = {{Beyond Pham's algorithm for joint diagonalization}},
  AUTHOR = {Ablin, Pierre A and Cardoso, Jean-Fran{\c c}ois and Gramfort, Alexandre},
  URL = {https://hal.archives-ouvertes.fr/hal-01936887},
  YEAR = {2019},
  MONTH = {April},
  PDF = {https://hal.archives-ouvertes.fr/hal-01936887/file/main.pdf},
  booktitle={European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
}

@INPROCEEDINGS{ablin-etal:18b, 
author={Ablin, P. and Fagot, D. and Wendt, H. and Gramfort, A. and Fevotte, C.}, 
booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title = "{A Quasi-Newton algorithm on the orthogonal manifold for NMF with transform learning}",
year={2019}, 
volume={}, 
number={}, 
pages={700-704}, 
keywords={Nonnegative matrix factorization (NMF);transform learning;source separation;non-convex optimization;manifolds;audio signal processing}, 
doi={10.1109/ICASSP.2019.8683291}, 
ISSN={2379-190X}, 
month={May},
URL = {https://arxiv.org/abs/1811.02225},
pdf = {https://arxiv.org/pdf/1811.02225.pdf}
}

@article {Grabot-etal:19,
  author = {Grabot, Laetitia and Kononowicz, Tadeusz W. and Dupr{\'e} la Tour, Tom and Gramfort, Alexandre and Doy{\`e}re, Val{\'e}rie and van Wassenhove, Virginie},
  title = {The Strength of Alpha{\textendash}Beta Oscillatory Coupling Predicts Motor Timing Precision},
  volume = {39},
  number = {17},
  pages = {3277--3291},
  year = {2019},
  doi = {10.1523/JNEUROSCI.2473-18.2018},
  publisher = {Society for Neuroscience},
  abstract = {Precise timing makes the difference between harmony and cacophony, but how the brain achieves precision during timing is unknown. In this study, human participants (7 females, 5 males) generated a time interval while being recorded with magnetoencephalography. Building on the proposal that the coupling of neural oscillations provides a temporal code for information processing in the brain, we tested whether the strength of oscillatory coupling was sensitive to self-generated temporal precision. On a per individual basis, we show the presence of alpha{\textendash}beta phase{\textendash}amplitude coupling whose strength was associated with the temporal precision of self-generated time intervals, not with their absolute duration. Our results provide evidence that active oscillatory coupling engages α oscillations in maintaining the precision of an endogenous temporal motor goal encoded in β power; the when of self-timed actions. We propose that oscillatory coupling indexes the variance of neuronal computations, which translates into the precision of an individual{\textquoteright}s behavioral performance.SIGNIFICANCE STATEMENT Which neural mechanisms enable precise volitional timing in the brain is unknown, yet accurate and precise timing is essential in every realm of life. In this study, we build on the hypothesis that neural oscillations, and their coupling across time scales, are essential for the coding and for the transmission of information in the brain. We show the presence of alpha{\textendash}beta phase{\textendash}amplitude coupling (α{\textendash}β PAC) whose strength was associated with the temporal precision of self-generated time intervals, not with their absolute duration. α{\textendash}β PAC indexes the temporal precision with which information is represented in an individual{\textquoteright}s brain. Our results link large-scale neuronal variability on the one hand, and individuals{\textquoteright} timing precision, on the other.},
  issn = {0270-6474},
  URL = {http://www.jneurosci.org/content/39/17/3277},
  eprint = {http://www.jneurosci.org/content/39/17/3277.full.pdf},
  journal = {Journal of Neuroscience}
}

@InProceedings{Ablin-etal:19a,
  title =    {Stochastic algorithms with descent guarantees for ICA},
  author =   {Ablin, Pierre and Gramfort, Alexandre and Cardoso, Jean-Fran\c{c}ois and Bach, Francis},
  booktitle =    {Proceedings of Machine Learning Research},
  pages =    {1564--1573},
  year =   {2019},
  editor =   {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume =   {89},
  series =   {Proceedings of Machine Learning Research},
  address =    {},
  month =    {16--18 Apr},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v89/ablin19a/ablin19a.pdf},
  url =    {http://proceedings.mlr.press/v89/ablin19a.html},
}

@inproceedings{dupre-etal:18,
title = {Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals},
author = {Dupr\'{e} la Tour, Tom and Moreau, Thomas and Jas, Mainak and Gramfort, Alexandre},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {3292--3302},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7590-multivariate-convolutional-sparse-coding-for-electromagnetic-brain-signals.pdf}
}

@inproceedings{janati-etal:19,
  title =    {Wasserstein regularization for sparse multi-task regression},
  author =   {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
  booktitle =    {Proceedings of Machine Learning Research},
  pages =    {1407--1416},
  year =   {2019},
  editor =   {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume =   {89},
  series =   {Proceedings of Machine Learning Research},
  address =    {},
  month =    {16--18 Apr},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v89/janati19a/janati19a.pdf},
  url =    {http://proceedings.mlr.press/v89/janati19a.html}
}

@article{jas-etal:18,
AUTHOR={Jas, Mainak and Larson, Eric and Engemann, Denis A. and Leppäkangas, Jaakko and Taulu, Samu and Hämäläinen, Matti and Gramfort, Alexandre},
TITLE={A Reproducible {MEG/EEG} Group Study With the MNE Software: Recommendations, Quality Assessments, and Good Practices},
JOURNAL={Frontiers in Neuroscience},
VOLUME={12},
PAGES={530},
YEAR={2018},
URL={https://www.frontiersin.org/article/10.3389/fnins.2018.00530},
DOI={10.3389/fnins.2018.00530},
ISSN={1662-453X},
Comment = {<a href="https://github.com/mne-tools/mne-biomag-group-demo">[Code]</a>},
}

@InProceedings{Massias_Gramfort_Salmon18,
  author  = {Massias, M. and Gramfort, A. and Salmon, J.},
  title = {Celer: a Fast Solver for the Lasso with Dual Extrapolation},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages = {3321--3330},
  year = {2018},
  volume = {80},
  Comment = {<a href="https://mathurinm.github.io/celer/">[Code]</a>},
  Url     = {https://arxiv.org/abs/1802.07481},
  pdf     = {https://arxiv.org/pdf/1802.07481}
}

@ARTICLE{ablin-etal:2017,
AUTHOR = {Ablin, Pierre and Cardoso, Jean-Francois and Gramfort, Alexandre},
journal={IEEE Transactions on Signal Processing},
title={Faster independent component analysis by preconditioning with Hessian approximations},
year={2018},
volume={66},
number={15},
pages={4040-4049},
keywords={Approximation algorithms;Brain modeling;Data models;Electronic mail;Neuroscience;Signal processing algorithms;Tensile stress;Blind source separation;Independent Component Analysis;maximum likelihood estimation;preconditioning;quasi-Newton methods;second order methods},
doi={10.1109/TSP.2018.2844203},
ISSN={1053-587X},
month={},
PDF = {https://hal.inria.fr/hal-01552340/file/quasi-newton-methods%20%286%29.pdf},
Comment = {<a href="https://github.com/pierreablin/picard">[Code]</a>},
}

@article{bekhti-etal:17,
  author = {Bekhti, Yousra and Lucka, Felix and Salmon, Joseph and Gramfort, Alexandre},
  title={A hierarchical Bayesian perspective on majorization-minimization for non-convex sparse regression: application to M/EEG source imaging},
  journal={Inverse Problems},
  url={http://iopscience.iop.org/article/10.1088/1361-6420/aac9b3/meta},
  year={2018},
  pdf={https://arxiv.org/pdf/1710.08747},
  Comment = {<a href="https://github.com/agramfort/bayes_mxne">[Code]</a>},
}

@InProceedings{ablin-etal:2018b,
author="Ablin, Pierre and Cardoso, Jean-Fran{\c{c}}ois and Gramfort, Alexandre",
editor="Deville, Yannick and Gannot, Sharon and Mason, Russell and Plumbley, Mark D. and Ward, Dominic",
title="Accelerating Likelihood Optimization for ICA on Real Signals",
booktitle="Latent Variable Analysis and Signal Separation (LVA-ICA)",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="151--160",
isbn="978-3-319-93764-9",
pdf={https://hal.inria.fr/hal-01822602/document},
Comment = {<a href="https://github.com/pierreablin/picard">[Code]</a>},
}

@InProceedings{ablin-etal:2018a,
  author = {Ablin, Pierre and Cardoso, Jean-Francois and Gramfort, Alexandre},
  title = "{Faster ICA under orthogonal constraint}",
  booktitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  address = {Calgary, Canada},
  year = {2018},
  month = Apr,
  pdf = {https://arxiv.org/pdf/1711.10873},
  Comment = {<a href="https://github.com/pierreablin/picard">[Code]</a>},
}

@inproceedings{duprelatour-etal:18,
  TITLE = {{Driver estimation in non-linear autoregressive models}},
  AUTHOR = {Dupré la Tour, Tom and Grenier, Yves and Gramfort, Alexandre},
  URL = {https://hal.archives-ouvertes.fr/hal-01696786},
  booktitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  ADDRESS = {Calgary, Canada},
  YEAR = {2018},
  MONTH = Apr,
  KEYWORDS = {cross-frequency coupling ; non-linear autoregressive models ; spectrum estimation ; electrophysiology},
  PDF = {https://hal.archives-ouvertes.fr/hal-01696786/file/duprelatour2018icassp.pdf},
  HAL_ID = {hal-01696786},
  HAL_VERSION = {v1},
}

@InProceedings{schiratti-etal:2018a,
  author = {Schiratti, Jean-Baptiste and Le Douget, Jean-Eudes and Le Van Quyen, Michel and Essid, Slim and Gramfort, Alexandre},
  title = "{An ensemble learning approach to detect epileptic seizures from long intracranial EEG recordings}",
  booktitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  address = {Calgary, Canada},
  year = {2018},
  month = Apr,
  pdf = {https://hal.archives-ouvertes.fr/hal-01724272/document},
  Comment = {<a href="https://github.com/mne-tools/mne-features">[Code]</a>},
}

@article{duprelatour-etal:2017b,
    author = {Dupré la Tour, Tom and Tallot, Lucille and Grabot, Laetitia and Doyere, Valerie and van Wassenhove, Virginie and Grenier, Yves and Gramfort, Alexandre},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Non-linear auto-regressive models for cross-frequency coupling in neural time series},
    year = {2017},
    month = {12},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pcbi.1005893},
    pages = {1-32},
    abstract = {Author summary Neural oscillations synchronize information across brain areas at various anatomical and temporal scales. Of particular relevance, slow fluctuations of brain activity have been shown to affect high frequency neural activity, by regulating the excitability level of neural populations. Such cross-frequency-coupling can take several forms. In the most frequently observed type, the power of high frequency activity is time-locked to a specific phase of slow frequency oscillations, yielding phase-amplitude-coupling (PAC). Even when readily observed in neural recordings, such non-linear coupling is particularly challenging to formally characterize. Typically, neuroscientists use band-pass filtering and Hilbert transforms with ad-hoc correlations. Here, we explicitly address current limitations and propose an alternative probabilistic signal modeling approach, for which statistical inference is fast and well-posed. To statistically model PAC, we propose to use non-linear auto-regressive models which estimate the spectral modulation of a signal conditionally to a driving signal. This conditional spectral analysis enables easy model selection and clear hypothesis-testing by using the likelihood of a given model. We demonstrate the advantage of the model-based approach on three datasets acquired in rats and in humans. We further provide novel neuroscientific insights on previously reported PAC phenomena, capturing two mechanisms in PAC: influence of amplitude and directionality estimation.},
    number = {12},
    doi = {10.1371/journal.pcbi.1005893},
    pdf={http://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1005893&type=printable}
}

@article{ndiaye-etal:17b,
  author  = {Ndiaye, Eugene and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
  title   = {Gap Safe Screening Rules for Sparsity Enforcing Penalties},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {128},
  pages   = {1-33},
  url     = {http://jmlr.org/papers/v18/16-577.html},
  pdf     = {http://jmlr.org/papers/volume18/16-577/16-577.pdf}
}

@article{ndiaye-etal:17,
  author={Ndiaye, Eugène and Fercoq, Olivier and Gramfort, Alexandre and Leclère, Vincent and Salmon, Joseph},
  title={Efficient Smoothed Concomitant Lasso Estimation for High Dimensional Regression},
  journal={Journal of Physics: Conference Series},
  volume={904},
  number={1},
  pages={012006},
  url={http://stacks.iop.org/1742-6596/904/i=1/a=012006},
  year={2017},
  abstract={In high dimensional settings, sparse structures are crucial for efficiency, both in term of memory, computation and performance. It is customary to consider ℓ 1 penalty to enforce sparsity in such scenarios. Sparsity enforcing methods, the Lasso being a canonical example, are popular candidates to address high dimension. For efficiency, they rely on tuning a parameter trading data fitting versus sparsity. For the Lasso theory to hold this tuning parameter should be proportional to the noise level, yet the latter is often unknown in practice. A possible remedy is to jointly optimize over the regression parameter as well as over the noise level. This has been considered under several names in the literature: Scaled-Lasso, Square-root Lasso, Concomitant Lasso estimation for instance, and could be of interest for uncertainty quantification. In this work, after illustrating numerical difficulties for the Concomitant Lasso formulation, we propose a modification we coined Smoothed Concomitant Lasso, aimed at increasing numerical stability. We propose an efficient and accurate solver leading to a computational cost no more expensive than the one for the Lasso. We leverage on standard ingredients behind the success of fast Lasso solvers: a coordinate descent algorithm, combined with safe screening rules to achieve speed efficiency, by eliminating early irrelevant features.},
  pdf={https://arxiv.org/pdf/1606.02702}
}

@inproceedings{bekhti-etal:17,
author={Bekhti, Yousra and Badeau, Roland and Gramfort, Alexandre},
booktitle={2017 25th European Signal Processing Conference (EUSIPCO)},
title={Hyperparameter estimation in maximum a posteriori regression using group sparsity with an application to brain imaging},
year={2017},
volume={},
number={},
pages={246-250},
keywords={Bayes methods;brain;convex programming;electroencephalography;inverse problems;iterative methods;maximum likelihood estimation;medical image processing;regression analysis;Bayesian inference;brain activations;brain imaging;faster algorithms;high-dimensional sparse synthesis models;hyperparameter estimation;inverse problem;nonconvex penalty;posteriori regression;recurrent problem;sparse regression models;Bayes methods;Brain modeling;Estimation;Europe;Inverse problems;Sensors;Signal processing},
doi={10.23919/EUSIPCO.2017.8081206},
ISSN={},
month={Aug},
pdf={https://hal.archives-ouvertes.fr/hal-01531238/document}
}

@article{NisoGalan-etal:18,
  Author = {Niso, Guiomar and Gorgolewski, Krzysztof J. and Bock, Elizabeth and Brooks, Teon L. and Flandin, Guillaume and Gramfort, Alexandre and Henson, Richard N. and Jas, Mainak and Litvak, Vladimir and T. Moreau, Jeremy and Oostenveld, Robert and Schoffelen, Jan-Mathijs and Tadel, Francois and Wexler, Joseph and Baillet, Sylvain},
  Date = {2018/06/19/online},
  Day = {19},
  Journal = {Scientific Data},
  Month = {06},
  Title = {MEG-BIDS, the brain imaging data structure extended to magnetoencephalography},
  Url = {http://dx.doi.org/10.1038/sdata.2018.110},
  Volume = {5},
  Year = {2018},
  pdf = {https://www.biorxiv.org/content/early/2017/08/08/172684.full.pdf},
}

@article{jas-etal:17b,
title = "Autoreject: Automated artifact rejection for MEG and EEG data",
author = "Jas, Mainak and Engemann, Denis A. and Bekhti, Yousra and Raimondo, Federico and Gramfort, Alexandre",
journal = "NeuroImage",
volume = "159",
pages = "417 - 429",
year = "2017",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2017.06.030",
url = "http://www.sciencedirect.com/science/article/pii/S1053811917305013",
pdf = "https://arxiv.org/pdf/1612.08194.pdf",
}

@InProceedings{massias-etal:2017,
title = 	 {Generalized Concomitant Multi-Task Lasso for Sparse Multimodal Regression},
author = 	 {Massias, Mathurin and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
booktitle = 	 {AISTATS},
pages = 	 {998--1007},
year = 	 {2018},
volume = 	 {84},
series = 	 {Proceedings of Machine Learning Research},
pdf = 	 {http://proceedings.mlr.press/v84/massias18a/massias18a.pdf},
}

@InProceedings{jas-etal:2017,
title = {Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding},
author = {Jas, Mainak and Dupr\'{e} la Tour, Tom and Simsekli, Umut and Gramfort, Alexandre},
booktitle = {Advances in Neural Information Processing Systems (NIPS) 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {1099--1108},
year = {2017},
publisher = {Curran Associates, Inc.},
pdf = {http://papers.nips.cc/paper/6710-learning-the-morphology-of-brain-signals-using-alpha-stable-convolutional-sparse-coding.pdf}
}

@misc{1703.07285,
Author = {Massias, Mathurin and Gramfort, Alexandre and Salmon, Joseph},
Title = {From safe screening rules to working sets for faster Lasso-type solvers},
Year = {2017},
Eprint = {arXiv:1703.07285},
pdf  = {http://arxiv.org/pdf/1703.07285.pdf},
}

@inproceedings{dupre-etal:2017,
    url = {https://hal.archives-ouvertes.fr/hal-01448603/},
    title = {{Parametric estimation of spectrum driven by an exogenous signal}},
    author = {Dupre la Tour, Tom and Grenier, Yves and Gramfort, Alexandre},
    booktitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
    address = {New Orleans, USA},
    year = {2017},
    month = Feb,
    pdf = {https://hal.archives-ouvertes.fr/hal-01448603/document},
}

@inproceedings{montoya-etal:2017,
    url = {https://hal.archives-ouvertes.fr/hal-01451432/},
    title = {{Caveats with stochastic gradient and maximum likelihood based ICA for EEG}},
    author = {Montoya-Martinez, Jair and Cardoso, Jean-François and Gramfort, Alexandre},
    booktitle = {International Conference on Latent Variable Analysis, Independent Component Analysis LVA-ICA},
    address = {Grenoble, France},
    year = {2017},
    month = Feb,
    pdf = {https://hal.archives-ouvertes.fr/hal-01451432/document},
}

@inproceedings{ndiaye-etal:16b,
   author = {Ndiaye, Eugène and Fercoq, Olivier and Gramfort, Alexandre and Salmon, J.},
    title = {{GAP} Safe Screening Rules for {Sparse-Group Lasso}},
    year = {2016},
    booktitle = {Proc. NIPS 2016},
    pdf  = {http://arxiv.org/pdf/1602.06225v1.pdf},
}

@article{ndiaye-etal:16a,
  author = {Ndiaye, Eugène and Fercoq, Olivier and Gramfort, Alexandre and Leclère, Vincent and Salmon, J.},
  title={Efficient Smoothed Concomitant Lasso Estimation for High Dimensional Regression},
  journal={Journal of Physics: Conference Series},
  volume={904},
  number={1},
  pages={012006},
  url={http://stacks.iop.org/1742-6596/904/i=1/a=012006},
  year={2017},
  abstract={In high dimensional settings, sparse structures are crucial for efficiency, both in term of memory, computation and performance. It is customary to consider ℓ 1 penalty to enforce sparsity in such scenarios. Sparsity enforcing methods, the Lasso being a canonical example, are popular candidates to address high dimension. For efficiency, they rely on tuning a parameter trading data fitting versus sparsity. For the Lasso theory to hold this tuning parameter should be proportional to the noise level, yet the latter is often unknown in practice. A possible remedy is to jointly optimize over the regression parameter as well as over the noise level. This has been considered under several names in the literature: Scaled-Lasso, Square-root Lasso, Concomitant Lasso estimation for instance, and could be of interest for uncertainty quantification. In this work, after illustrating numerical difficulties for the Concomitant Lasso formulation, we propose a modification we coined Smoothed Concomitant Lasso, aimed at increasing numerical stability. We propose an efficient and accurate solver leading to a computational cost no more expensive than the one for the Lasso. We leverage on standard ingredients behind the success of fast Lasso solvers: a coordinate descent algorithm, combined with safe screening rules to achieve speed efficiency, by eliminating early irrelevant features.},
  pdf  = {https://arxiv.org/pdf/1606.02702v1.pdf}
}
