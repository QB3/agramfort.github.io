@InProceedings{janati-etal:20a,
  title =    {Spatio-temporal alignments: Optimal transport through space and time},
  author =       {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
  pages =    {1695--1704},
  year =   {2020},
  editor =   {Silvia Chiappa and Roberto Calandra},
  volume =   {108},
  booktitle =    {AISTATS},
  series =   {Proceedings of Machine Learning Research},
  address =    {Online},
  month =    {26--28 Aug},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v108/janati20a/janati20a.pdf},
  url =    {http://proceedings.mlr.press/v108/janati20a.html},
  abstract =   {Comparing data defined over space and time is notoriously hard. It involves quantifying both spatial and temporal variability while taking into account the chronological structure of the data. Dynamic Time Warping (DTW) computes a minimal cost alignment between time series that preserves the chronological order but is inherently blind to spatio-temporal shifts. In this paper, we propose Spatio-Temporal Alignments (STA), a new differentiable formulation of DTW that captures spatial and temporal variability. Spatial differences between time samples are captured using regularized Optimal transport. While temporal alignment cost exploits a smooth variant of DTW called soft-DTW. We show how smoothing DTW leads to alignment costs that increase quadratically with time shifts. The costs are expressed using an unbalanced Wasserstein distance to cope with observations that are not probabilities. Experiments on handwritten letters and brain imaging data confirm our theoretical findings and illustrate the effectiveness of STA as a dissimilarity for spatio-temporal data.}
}

@InProceedings{janati-etal:20b,
author =   {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
booktitle={Proc. ICML 2020},
title={Debiased Sinkhorn Barycenters},
year={2020},
month={July},
pdf={https://proceedings.icml.cc/static/paper_files/icml/2020/1584-Paper.pdf},
}

@InProceedings{bertrand-etal:20,
  author    = {Bertrand, Quentin and Klopfenstein, Quentin and Blondel, Mathieu and Vaiter, Samuel and Gramfort, Alexandre and Salmon, Joseph},
  title     = {Implicit differentiation of Lasso-type models for hyperparameter optimization},
  booktitle={Proc. ICML 2020},
  year      = {2020},
  month={July},
  url       = {https://arxiv.org/abs/2002.08943},
  pdf       = {https://proceedings.icml.cc/static/paper_files/icml/2020/1831-Paper.pdf},
}

@article{engemann-etal:20,
title = {Combining magnetoencephalography with magnetic resonance imaging enhances learning of surrogate-biomarkers},
author = {Engemann, Denis A and Kozynets, Oleh and Sabbagh, David and Lemaître, Guillaume and Varoquaux, Gael and Liem, Franziskus and Gramfort, Alexandre},
editor = {Shackman, Alexander and de Lange, Floris P and Tsvetanov, Kamen and Trujillo-Barreto, Nelson},
volume = 9,
year = 2020,
month = {may},
pub_date = {2020-05-19},
pages = {e54055},
citation = {eLife 2020;9:e54055},
doi = {10.7554/eLife.54055},
url = {https://doi.org/10.7554/eLife.54055},
abstract = {Electrophysiological methods, that is M/EEG, provide unique views into brain health. Yet, when building predictive models from brain data, it is often unclear how electrophysiology should be combined with other neuroimaging methods. Information can be redundant, useful common representations of multimodal data may not be obvious and multimodal data collection can be medically contraindicated, which reduces applicability. Here, we propose a multimodal model to robustly combine MEG, MRI and fMRI for prediction. We focus on age prediction as a surrogate biomarker in 674 subjects from the Cam-CAN dataset. Strikingly, MEG, fMRI and MRI showed additive effects supporting distinct brain-behavior associations. Moreover, the contribution of MEG was best explained by cortical power spectra between 8 and 30 Hz. Finally, we demonstrate that the model preserves benefits of stacking when some data is missing. The proposed framework, hence, enables multimodal learning for a wide range of biomarkers from diverse types of brain signals.},
keywords = {biomarker, aging, magnetic resonance imaging, magnetoencephalogrphy, oscillations, machine learning},
journal = {eLife},
issn = {2050-084X},
publisher = {eLife Sciences Publications, Ltd},
}

@article{sabbagh-etal:20,
title = "Predictive regression modeling with MEG/EEG: from source power to signals and cognitive states",
journal = "NeuroImage",
volume = "222",
pages = "116893",
year = "2020",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2020.116893",
url = "http://www.sciencedirect.com/science/article/pii/S1053811920303797",
author = "Sabbagh, David and Ablin, Pierre and Varoquaux, Gaël and Gramfort, Alexandre and Engemann, Denis A.",
keywords = "MEG/EEG, Neuronal oscillations, Machine learning, Covariance, Spatial filters, Riemannian geometry",
abstract = "Predicting biomedical outcomes from Magnetoencephalography and Electroencephalography (M/EEG) is central to applications like decoding, brain-computer-interfaces (BCI) or biomarker development and is facilitated by supervised machine learning. Yet, most of the literature is concerned with classification of outcomes defined at the event-level. Here, we focus on predicting continuous outcomes from M/EEG signal defined at the subject-level, and analyze about 600 MEG recordings from Cam-CAN dataset and about 1000 EEG recordings from TUH dataset. Considering different generative mechanisms for M/EEG signals and the biomedical outcome, we propose statistically-consistent predictive models that avoid source-reconstruction based on the covariance as representation. Our mathematical analysis and ground-truth simulations demonstrated that consistent function approximation can be obtained with supervised spatial filtering or by embedding with Riemannian geometry. Additional simulations revealed that Riemannian methods were more robust to model violations, in particular geometric distortions induced by individual anatomy. To estimate the relative contribution of brain dynamics and anatomy to prediction performance, we propose a novel model inspection procedure based on biophysical forward modeling. Applied to prediction of outcomes at the subject-level, the analysis revealed that the Riemannian model better exploited anatomical information while sensitivity to brain dynamics was similar across methods. We then probed the robustness of the models across different data cleaning options. Environmental denoising was globally important but Riemannian models were strikingly robust and continued performing well even without preprocessing. Our results suggest each method has its niche: supervised spatial filtering is practical for event-level prediction while the Riemannian model may enable simple end-to-end learning."
}


@article{jaiswal-etal:20,
title = "Comparison of beamformer implementations for MEG source localization",
journal = "NeuroImage",
pages = "116797",
year = "2020",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2020.116797",
url = "http://www.sciencedirect.com/science/article/pii/S1053811920302846",
author = "Jaiswal, Amit and Nenonen, Jukka and Stenroos, Matti and Gramfort, Alexandre and Dalal, Sarang S. and Westner, Britta U. and Litvak, Vladimir and Mosher, John C. and Schoffelen, Jan-Mathijs and Witton, Caroline and Oostenveld, Robert and Parkkonen, Lauri",
keywords = "MEG, EEG, source modeling, beamformers, LCMV, open-source analysis toolbox",
}

@article{janati-etal:2020,
title = "Multi-subject MEG/EEG source imaging with sparse multi-task regression",
journal = "NeuroImage",
volume = "220",
pages = "116847",
year = "2020",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2020.116847",
url = "http://www.sciencedirect.com/science/article/pii/S1053811920303347",
pdf = "https://arxiv.org/pdf/1910.01914.pdf",
author = "Janati, Hicham and Bazeille, Thomas and Thirion, Bertrand and Cuturi, Marco and Gramfort, Alexandre",
keywords = "Brain, Inverse modeling, EEG / MEG source imaging",
abstract = "Magnetoencephalography and electroencephalography (M/EEG) are non-invasive modalities that measure the weak electromagnetic fields generated by neural activity. Estimating the location and magnitude of the current sources that generated these electromagnetic fields is an inverse problem. Although it can be cast as a linear regression, this problem is severely ill-posed as the number of observations, which equals the number of sensors, is small. When considering a group study, a common approach consists in carrying out the regression tasks independently for each subject using techniques such as MNE or sLORETA. An alternative is to jointly localize sources for all subjects taken together, while enforcing some similarity between them. By pooling S subjects in a single joint regression, the number of observations is S times larger, potentially making the problem better posed and offering the ability to identify more sources with greater precision. Here we show how the coupling of the different regression problems can be done through a multi-task regularization that promotes focal source estimates. To take into account intersubject variabilities, we propose the Minimum Wasserstein Estimates (MWE). Thanks to a new joint regression method based on optimal transport (OT) metrics, MWE does not enforce perfect overlap of activation foci for all subjects but rather promotes spatial proximity on the cortical mantle. Besides, by estimating the noise level of each subject, MWE copes with the subject-specific signal-to-noise ratios with only one regularization parameter. On realistic simulations, MWE decreases the localization error by up to 4 ​mm per source compared to individual solutions. Experiments on the Cam-CAN dataset show improvements in spatial specificity in population imaging compared to individual models such as dSPM as well as a state-of-the-art Bayesian group level model. Our analysis of a multimodal dataset shows how multi-subject source localization reduces the gap between MEG and fMRI for brain mapping."
}

@article{appelhoff_mne-bids:2019,
    title = {{MNE}-{BIDS}: {Organizing} electrophysiological data into
the {BIDS} format and facilitating their analysis},
    volume = {4},
    issn = {2475-9066},
    shorttitle = {{MNE}-{BIDS}},
    url = {https://joss.theoj.org/papers/10.21105/joss.01896},
    doi = {10.21105/joss.01896},
    language = {en},
    number = {44},
    urldate = {2019-12-19},
    journal = {Journal of Open Source Software},
    author = {Appelhoff, Stefan and Sanderson, Matthew and Brooks, Teon and Vliet, Marijn van and Quentin, Romain and Holdgraf, Chris and Chaumon, Maximilien and Mikulan, Ezequiel and Tavabi, Kambiz and Höchenberger, Richard and Welke, Dominik and Brunner, Clemens and Rockhill, Alexander and Larson, Eric and Gramfort, Alexandre and Jas, Mainak},
    month = dec,
    year = {2019},
    pages = {1896},
    Comment = {<a href="https://github.com/mne-tools/mne-bids">[Code]</a>},
}


@inproceedings{sabbagh-etal:2019,
  title = {Manifold-regression to predict from MEG/EEG brain signals without source modeling},
  author = {Sabbagh, David and Ablin, Pierre and Varoquaux, Gael and Gramfort, Alexandre and Engemann, Denis A.},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {7321--7332},
  year = {2019},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/8952-manifold-regression-to-predict-from-megeeg-brain-signals-without-source-modeling.pdf},
  Comment = {<a href="https://github.com/DavidSabbagh/NeurIPS19_manifold-regression-meeg">[Code]</a>},
}


@unpublished{ablin:hal-02140383,
  TITLE = {{Learning step sizes for unfolded sparse coding}},
  AUTHOR = {Ablin, Pierre and Moreau, Thomas and Massias, Mathurin and Gramfort, Alexandre},
  URL = {https://hal.inria.fr/hal-02140383},
  NOTE = {working paper or preprint},
  YEAR = {2019},
  MONTH = May,
  PDF = {https://hal.inria.fr/hal-02140383/file/main.pdf},
  HAL_ID = {hal-02140383},
  HAL_VERSION = {v1},
}

@unpublished{massias:hal-02263500,
  TITLE = {{Dual Extrapolation for Sparse Generalized Linear Models}},
  AUTHOR = {Massias, Mathurin and Vaiter, Samuel and Gramfort, Alexandre and Salmon, Joseph},
  URL = {https://hal.archives-ouvertes.fr/hal-02263500},
  NOTE = {working paper or preprint},
  YEAR = {2019},
  MONTH = Aug,
  PDF = {https://hal.archives-ouvertes.fr/hal-02263500/file/main.pdf},
  HAL_ID = {hal-02263500},
  HAL_VERSION = {v1},
}

@InProceedings{janati-etal:2019b,
author = {Janati, H. and Bazeille, T. and Thirion, B. and Cuturi, M. and Gramfort, A.},
editor="Chung, Albert C. S.
and Gee, James C.
and Yushkevich, Paul A.
and Bao, Siqi",
title="Group Level MEG/EEG Source Imaging via Optimal Transport: Minimum Wasserstein Estimates",
booktitle="Information Processing in Medical Imaging",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="743--754",
isbn="978-3-030-20351-1",
pdf = {https://arxiv.org/pdf/1902.04812.pdf},
}

@ARTICLE{2019arXiv190202509B,
   author = {Bertrand, Q. and Massias, M. and Gramfort, A. and Salmon, J.},
    title = "{Concomitant Lasso with Repetitions (CLaR): beyond averaging multiple realizations of heteroscedastic noise}",
  journal = {arXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1902.02509},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Applications},
     year = 2019,
    month = feb,
   adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190202509B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  pdf = {https://arxiv.org/pdf/1902.02509.pdf},
}

@ARTICLE{2019arXiv190109235M,
   author = {Moreau, T. and Gramfort, A.},
    title = "{Distributed Convolutional Dictionary Learning (DiCoDiLe): Pattern Discovery in Large Images and Signals}",
  journal = {arXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1901.09235},
 keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning},
     year = 2019,
    month = jan,
   adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190109235M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  pdf = {https://arxiv.org/pdf/1901.09235.pdf},
}

@INPROCEEDINGS{ablin-etal:18c,
  TITLE = {{Beyond Pham's algorithm for joint diagonalization}},
  AUTHOR = {Ablin, Pierre A and Cardoso, Jean-Fran{\c c}ois and Gramfort, Alexandre},
  URL = {https://hal.archives-ouvertes.fr/hal-01936887},
  YEAR = {2019},
  MONTH = {April},
  PDF = {https://hal.archives-ouvertes.fr/hal-01936887/file/main.pdf},
  booktitle={European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
}

@INPROCEEDINGS{ablin-etal:18b, 
author={Ablin, P. and Fagot, D. and Wendt, H. and Gramfort, A. and Fevotte, C.}, 
booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title = "{A Quasi-Newton algorithm on the orthogonal manifold for NMF with transform learning}",
year={2019}, 
volume={}, 
number={}, 
pages={700-704}, 
keywords={Nonnegative matrix factorization (NMF);transform learning;source separation;non-convex optimization;manifolds;audio signal processing}, 
doi={10.1109/ICASSP.2019.8683291}, 
ISSN={2379-190X}, 
month={May},
URL = {https://arxiv.org/abs/1811.02225},
pdf = {https://arxiv.org/pdf/1811.02225.pdf}
}

@article {Grabot-etal:19,
  author = {Grabot, Laetitia and Kononowicz, Tadeusz W. and Dupr{\'e} la Tour, Tom and Gramfort, Alexandre and Doy{\`e}re, Val{\'e}rie and van Wassenhove, Virginie},
  title = {The Strength of Alpha{\textendash}Beta Oscillatory Coupling Predicts Motor Timing Precision},
  volume = {39},
  number = {17},
  pages = {3277--3291},
  year = {2019},
  doi = {10.1523/JNEUROSCI.2473-18.2018},
  publisher = {Society for Neuroscience},
  abstract = {Precise timing makes the difference between harmony and cacophony, but how the brain achieves precision during timing is unknown. In this study, human participants (7 females, 5 males) generated a time interval while being recorded with magnetoencephalography. Building on the proposal that the coupling of neural oscillations provides a temporal code for information processing in the brain, we tested whether the strength of oscillatory coupling was sensitive to self-generated temporal precision. On a per individual basis, we show the presence of alpha{\textendash}beta phase{\textendash}amplitude coupling whose strength was associated with the temporal precision of self-generated time intervals, not with their absolute duration. Our results provide evidence that active oscillatory coupling engages α oscillations in maintaining the precision of an endogenous temporal motor goal encoded in β power; the when of self-timed actions. We propose that oscillatory coupling indexes the variance of neuronal computations, which translates into the precision of an individual{\textquoteright}s behavioral performance.SIGNIFICANCE STATEMENT Which neural mechanisms enable precise volitional timing in the brain is unknown, yet accurate and precise timing is essential in every realm of life. In this study, we build on the hypothesis that neural oscillations, and their coupling across time scales, are essential for the coding and for the transmission of information in the brain. We show the presence of alpha{\textendash}beta phase{\textendash}amplitude coupling (α{\textendash}β PAC) whose strength was associated with the temporal precision of self-generated time intervals, not with their absolute duration. α{\textendash}β PAC indexes the temporal precision with which information is represented in an individual{\textquoteright}s brain. Our results link large-scale neuronal variability on the one hand, and individuals{\textquoteright} timing precision, on the other.},
  issn = {0270-6474},
  URL = {http://www.jneurosci.org/content/39/17/3277},
  eprint = {http://www.jneurosci.org/content/39/17/3277.full.pdf},
  journal = {Journal of Neuroscience}
}

@InProceedings{Ablin-etal:19a,
  title =    {Stochastic algorithms with descent guarantees for ICA},
  author =   {Ablin, Pierre and Gramfort, Alexandre and Cardoso, Jean-Fran\c{c}ois and Bach, Francis},
  booktitle =    {Proceedings of Machine Learning Research},
  pages =    {1564--1573},
  year =   {2019},
  editor =   {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume =   {89},
  series =   {Proceedings of Machine Learning Research},
  address =    {},
  month =    {16--18 Apr},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v89/ablin19a/ablin19a.pdf},
  url =    {http://proceedings.mlr.press/v89/ablin19a.html},
}

@inproceedings{dupre-etal:18,
title = {Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals},
author = {Dupr\'{e} la Tour, Tom and Moreau, Thomas and Jas, Mainak and Gramfort, Alexandre},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {3292--3302},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7590-multivariate-convolutional-sparse-coding-for-electromagnetic-brain-signals.pdf}
}

@inproceedings{janati-etal:19,
  title =    {Wasserstein regularization for sparse multi-task regression},
  author =   {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
  booktitle =    {Proceedings of Machine Learning Research},
  pages =    {1407--1416},
  year =   {2019},
  editor =   {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume =   {89},
  series =   {Proceedings of Machine Learning Research},
  address =    {},
  month =    {16--18 Apr},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v89/janati19a/janati19a.pdf},
  url =    {http://proceedings.mlr.press/v89/janati19a.html}
}

@article{jas-etal:18,
AUTHOR={Jas, Mainak and Larson, Eric and Engemann, Denis A. and Leppäkangas, Jaakko and Taulu, Samu and Hämäläinen, Matti and Gramfort, Alexandre},
TITLE={A Reproducible {MEG/EEG} Group Study With the MNE Software: Recommendations, Quality Assessments, and Good Practices},
JOURNAL={Frontiers in Neuroscience},
VOLUME={12},
PAGES={530},
YEAR={2018},
URL={https://www.frontiersin.org/article/10.3389/fnins.2018.00530},
DOI={10.3389/fnins.2018.00530},
ISSN={1662-453X},
Comment = {<a href="https://github.com/mne-tools/mne-biomag-group-demo">[Code]</a>},
}

@InProceedings{Massias_Gramfort_Salmon18,
  author  = {Massias, M. and Gramfort, A. and Salmon, J.},
  title = {Celer: a Fast Solver for the Lasso with Dual Extrapolation},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages = {3321--3330},
  year = {2018},
  volume = {80},
  Comment = {<a href="https://mathurinm.github.io/celer/">[Code]</a>},
  Url     = {https://arxiv.org/abs/1802.07481},
  pdf     = {https://arxiv.org/pdf/1802.07481}
}

@ARTICLE{ablin-etal:2017,
AUTHOR = {Ablin, Pierre and Cardoso, Jean-Francois and Gramfort, Alexandre},
journal={IEEE Transactions on Signal Processing},
title={Faster independent component analysis by preconditioning with Hessian approximations},
year={2018},
volume={66},
number={15},
pages={4040-4049},
keywords={Approximation algorithms;Brain modeling;Data models;Electronic mail;Neuroscience;Signal processing algorithms;Tensile stress;Blind source separation;Independent Component Analysis;maximum likelihood estimation;preconditioning;quasi-Newton methods;second order methods},
doi={10.1109/TSP.2018.2844203},
ISSN={1053-587X},
month={},
PDF = {https://hal.inria.fr/hal-01552340/file/quasi-newton-methods%20%286%29.pdf},
Comment = {<a href="https://github.com/pierreablin/picard">[Code]</a>},
}

@article{bekhti-etal:17,
  author = {Bekhti, Yousra and Lucka, Felix and Salmon, Joseph and Gramfort, Alexandre},
  title={A hierarchical Bayesian perspective on majorization-minimization for non-convex sparse regression: application to M/EEG source imaging},
  journal={Inverse Problems},
  url={http://iopscience.iop.org/article/10.1088/1361-6420/aac9b3/meta},
  year={2018},
  pdf={https://arxiv.org/pdf/1710.08747},
  Comment = {<a href="https://github.com/agramfort/bayes_mxne">[Code]</a>},
}

@InProceedings{ablin-etal:2018b,
author="Ablin, Pierre and Cardoso, Jean-Fran{\c{c}}ois and Gramfort, Alexandre",
editor="Deville, Yannick and Gannot, Sharon and Mason, Russell and Plumbley, Mark D. and Ward, Dominic",
title="Accelerating Likelihood Optimization for ICA on Real Signals",
booktitle="Latent Variable Analysis and Signal Separation (LVA-ICA)",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="151--160",
isbn="978-3-319-93764-9",
pdf={https://hal.inria.fr/hal-01822602/document},
Comment = {<a href="https://github.com/pierreablin/picard">[Code]</a>},
}

@InProceedings{ablin-etal:2018a,
  author = {Ablin, Pierre and Cardoso, Jean-Francois and Gramfort, Alexandre},
  title = "{Faster ICA under orthogonal constraint}",
  booktitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  address = {Calgary, Canada},
  year = {2018},
  month = Apr,
  pdf = {https://arxiv.org/pdf/1711.10873},
  Comment = {<a href="https://github.com/pierreablin/picard">[Code]</a>},
}

@inproceedings{duprelatour-etal:18,
  TITLE = {{Driver estimation in non-linear autoregressive models}},
  AUTHOR = {Dupré la Tour, Tom and Grenier, Yves and Gramfort, Alexandre},
  URL = {https://hal.archives-ouvertes.fr/hal-01696786},
  booktitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  ADDRESS = {Calgary, Canada},
  YEAR = {2018},
  MONTH = Apr,
  KEYWORDS = {cross-frequency coupling ; non-linear autoregressive models ; spectrum estimation ; electrophysiology},
  PDF = {https://hal.archives-ouvertes.fr/hal-01696786/file/duprelatour2018icassp.pdf},
  HAL_ID = {hal-01696786},
  HAL_VERSION = {v1},
}

@InProceedings{schiratti-etal:2018a,
  author = {Schiratti, Jean-Baptiste and Le Douget, Jean-Eudes and Le Van Quyen, Michel and Essid, Slim and Gramfort, Alexandre},
  title = "{An ensemble learning approach to detect epileptic seizures from long intracranial EEG recordings}",
  booktitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  address = {Calgary, Canada},
  year = {2018},
  month = Apr,
  pdf = {https://hal.archives-ouvertes.fr/hal-01724272/document},
  Comment = {<a href="https://github.com/mne-tools/mne-features">[Code]</a>},
}

@article{duprelatour-etal:2017b,
    author = {Dupré la Tour, Tom and Tallot, Lucille and Grabot, Laetitia and Doyere, Valerie and van Wassenhove, Virginie and Grenier, Yves and Gramfort, Alexandre},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Non-linear auto-regressive models for cross-frequency coupling in neural time series},
    year = {2017},
    month = {12},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pcbi.1005893},
    pages = {1-32},
    abstract = {Author summary Neural oscillations synchronize information across brain areas at various anatomical and temporal scales. Of particular relevance, slow fluctuations of brain activity have been shown to affect high frequency neural activity, by regulating the excitability level of neural populations. Such cross-frequency-coupling can take several forms. In the most frequently observed type, the power of high frequency activity is time-locked to a specific phase of slow frequency oscillations, yielding phase-amplitude-coupling (PAC). Even when readily observed in neural recordings, such non-linear coupling is particularly challenging to formally characterize. Typically, neuroscientists use band-pass filtering and Hilbert transforms with ad-hoc correlations. Here, we explicitly address current limitations and propose an alternative probabilistic signal modeling approach, for which statistical inference is fast and well-posed. To statistically model PAC, we propose to use non-linear auto-regressive models which estimate the spectral modulation of a signal conditionally to a driving signal. This conditional spectral analysis enables easy model selection and clear hypothesis-testing by using the likelihood of a given model. We demonstrate the advantage of the model-based approach on three datasets acquired in rats and in humans. We further provide novel neuroscientific insights on previously reported PAC phenomena, capturing two mechanisms in PAC: influence of amplitude and directionality estimation.},
    number = {12},
    doi = {10.1371/journal.pcbi.1005893},
    pdf={http://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1005893&type=printable}
}

@article{ndiaye-etal:17b,
  author  = {Ndiaye, Eugene and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
  title   = {Gap Safe Screening Rules for Sparsity Enforcing Penalties},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {128},
  pages   = {1-33},
  url     = {http://jmlr.org/papers/v18/16-577.html},
  pdf     = {http://jmlr.org/papers/volume18/16-577/16-577.pdf}
}

@article{ndiaye-etal:17,
  author={Ndiaye, Eugène and Fercoq, Olivier and Gramfort, Alexandre and Leclère, Vincent and Salmon, Joseph},
  title={Efficient Smoothed Concomitant Lasso Estimation for High Dimensional Regression},
  journal={Journal of Physics: Conference Series},
  volume={904},
  number={1},
  pages={012006},
  url={http://stacks.iop.org/1742-6596/904/i=1/a=012006},
  year={2017},
  abstract={In high dimensional settings, sparse structures are crucial for efficiency, both in term of memory, computation and performance. It is customary to consider ℓ 1 penalty to enforce sparsity in such scenarios. Sparsity enforcing methods, the Lasso being a canonical example, are popular candidates to address high dimension. For efficiency, they rely on tuning a parameter trading data fitting versus sparsity. For the Lasso theory to hold this tuning parameter should be proportional to the noise level, yet the latter is often unknown in practice. A possible remedy is to jointly optimize over the regression parameter as well as over the noise level. This has been considered under several names in the literature: Scaled-Lasso, Square-root Lasso, Concomitant Lasso estimation for instance, and could be of interest for uncertainty quantification. In this work, after illustrating numerical difficulties for the Concomitant Lasso formulation, we propose a modification we coined Smoothed Concomitant Lasso, aimed at increasing numerical stability. We propose an efficient and accurate solver leading to a computational cost no more expensive than the one for the Lasso. We leverage on standard ingredients behind the success of fast Lasso solvers: a coordinate descent algorithm, combined with safe screening rules to achieve speed efficiency, by eliminating early irrelevant features.},
  pdf={https://arxiv.org/pdf/1606.02702}
}

@inproceedings{bekhti-etal:17,
author={Bekhti, Yousra and Badeau, Roland and Gramfort, Alexandre},
booktitle={2017 25th European Signal Processing Conference (EUSIPCO)},
title={Hyperparameter estimation in maximum a posteriori regression using group sparsity with an application to brain imaging},
year={2017},
volume={},
number={},
pages={246-250},
keywords={Bayes methods;brain;convex programming;electroencephalography;inverse problems;iterative methods;maximum likelihood estimation;medical image processing;regression analysis;Bayesian inference;brain activations;brain imaging;faster algorithms;high-dimensional sparse synthesis models;hyperparameter estimation;inverse problem;nonconvex penalty;posteriori regression;recurrent problem;sparse regression models;Bayes methods;Brain modeling;Estimation;Europe;Inverse problems;Sensors;Signal processing},
doi={10.23919/EUSIPCO.2017.8081206},
ISSN={},
month={Aug},
pdf={https://hal.archives-ouvertes.fr/hal-01531238/document}
}

@article{NisoGalan-etal:18,
  Author = {Niso, Guiomar and Gorgolewski, Krzysztof J. and Bock, Elizabeth and Brooks, Teon L. and Flandin, Guillaume and Gramfort, Alexandre and Henson, Richard N. and Jas, Mainak and Litvak, Vladimir and T. Moreau, Jeremy and Oostenveld, Robert and Schoffelen, Jan-Mathijs and Tadel, Francois and Wexler, Joseph and Baillet, Sylvain},
  Date = {2018/06/19/online},
  Day = {19},
  Journal = {Scientific Data},
  Month = {06},
  Title = {MEG-BIDS, the brain imaging data structure extended to magnetoencephalography},
  Url = {http://dx.doi.org/10.1038/sdata.2018.110},
  Volume = {5},
  Year = {2018},
  pdf = {https://www.biorxiv.org/content/early/2017/08/08/172684.full.pdf},
}

@article{jas-etal:17b,
title = "Autoreject: Automated artifact rejection for MEG and EEG data",
author = "Jas, Mainak and Engemann, Denis A. and Bekhti, Yousra and Raimondo, Federico and Gramfort, Alexandre",
journal = "NeuroImage",
volume = "159",
pages = "417 - 429",
year = "2017",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2017.06.030",
url = "http://www.sciencedirect.com/science/article/pii/S1053811917305013",
pdf = "https://arxiv.org/pdf/1612.08194.pdf",
}

@InProceedings{massias-etal:2017,
title = 	 {Generalized Concomitant Multi-Task Lasso for Sparse Multimodal Regression},
author = 	 {Massias, Mathurin and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
booktitle = 	 {AISTATS},
pages = 	 {998--1007},
year = 	 {2018},
volume = 	 {84},
series = 	 {Proceedings of Machine Learning Research},
pdf = 	 {http://proceedings.mlr.press/v84/massias18a/massias18a.pdf},
}

@InProceedings{jas-etal:2017,
title = {Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding},
author = {Jas, Mainak and Dupr\'{e} la Tour, Tom and Simsekli, Umut and Gramfort, Alexandre},
booktitle = {Advances in Neural Information Processing Systems (NIPS) 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {1099--1108},
year = {2017},
publisher = {Curran Associates, Inc.},
pdf = {http://papers.nips.cc/paper/6710-learning-the-morphology-of-brain-signals-using-alpha-stable-convolutional-sparse-coding.pdf}
}

@misc{1703.07285,
Author = {Massias, Mathurin and Gramfort, Alexandre and Salmon, Joseph},
Title = {From safe screening rules to working sets for faster Lasso-type solvers},
Year = {2017},
Eprint = {arXiv:1703.07285},
pdf  = {http://arxiv.org/pdf/1703.07285.pdf},
}

@inproceedings{dupre-etal:2017,
    url = {https://hal.archives-ouvertes.fr/hal-01448603/},
    title = {{Parametric estimation of spectrum driven by an exogenous signal}},
    author = {Dupre la Tour, Tom and Grenier, Yves and Gramfort, Alexandre},
    booktitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
    address = {New Orleans, USA},
    year = {2017},
    month = Feb,
    pdf = {https://hal.archives-ouvertes.fr/hal-01448603/document},
}

@inproceedings{montoya-etal:2017,
    url = {https://hal.archives-ouvertes.fr/hal-01451432/},
    title = {{Caveats with stochastic gradient and maximum likelihood based ICA for EEG}},
    author = {Montoya-Martinez, Jair and Cardoso, Jean-François and Gramfort, Alexandre},
    booktitle = {International Conference on Latent Variable Analysis, Independent Component Analysis LVA-ICA},
    address = {Grenoble, France},
    year = {2017},
    month = Feb,
    pdf = {https://hal.archives-ouvertes.fr/hal-01451432/document},
}

@inproceedings{ndiaye-etal:16b,
   author = {Ndiaye, Eugène and Fercoq, Olivier and Gramfort, Alexandre and Salmon, J.},
    title = {{GAP} Safe Screening Rules for {Sparse-Group Lasso}},
    year = {2016},
    booktitle = {Proc. NIPS 2016},
    pdf  = {http://arxiv.org/pdf/1602.06225v1.pdf},
}

@article{ndiaye-etal:16a,
  author = {Ndiaye, Eugène and Fercoq, Olivier and Gramfort, Alexandre and Leclère, Vincent and Salmon, J.},
  title={Efficient Smoothed Concomitant Lasso Estimation for High Dimensional Regression},
  journal={Journal of Physics: Conference Series},
  volume={904},
  number={1},
  pages={012006},
  url={http://stacks.iop.org/1742-6596/904/i=1/a=012006},
  year={2017},
  abstract={In high dimensional settings, sparse structures are crucial for efficiency, both in term of memory, computation and performance. It is customary to consider ℓ 1 penalty to enforce sparsity in such scenarios. Sparsity enforcing methods, the Lasso being a canonical example, are popular candidates to address high dimension. For efficiency, they rely on tuning a parameter trading data fitting versus sparsity. For the Lasso theory to hold this tuning parameter should be proportional to the noise level, yet the latter is often unknown in practice. A possible remedy is to jointly optimize over the regression parameter as well as over the noise level. This has been considered under several names in the literature: Scaled-Lasso, Square-root Lasso, Concomitant Lasso estimation for instance, and could be of interest for uncertainty quantification. In this work, after illustrating numerical difficulties for the Concomitant Lasso formulation, we propose a modification we coined Smoothed Concomitant Lasso, aimed at increasing numerical stability. We propose an efficient and accurate solver leading to a computational cost no more expensive than the one for the Lasso. We leverage on standard ingredients behind the success of fast Lasso solvers: a coordinate descent algorithm, combined with safe screening rules to achieve speed efficiency, by eliminating early irrelevant features.},
  pdf  = {https://arxiv.org/pdf/1606.02702v1.pdf}
}
